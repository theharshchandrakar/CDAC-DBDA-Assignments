{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6d73c7",
   "metadata": {},
   "source": [
    "# Assignment 2 - PML\n",
    "\n",
    " Harsh Chandrakar\n",
    " \n",
    " (PRN-250840125020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16808a97",
   "metadata": {},
   "source": [
    "\n",
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664ce6e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingClassifier\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_loss\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load Data\n",
    "df_glass = pd.read_csv(\"Glass.csv\")\n",
    "X = df_glass.drop(\"Type\", axis=1)\n",
    "y = df_glass[\"Type\"]\n",
    "\n",
    "# Encode Target (Classes are 1, 2, 3, 5, 6, 7)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "num_classes = len(np.unique(y_enc))\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_enc, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Neural Network (TensorFlow/Keras)\n",
    "model_nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_nn.fit(X_train, y_train, epochs=50, verbose=0, batch_size=16)\n",
    "\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "loss_nn = log_loss(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Log Loss: {loss_nn:.4f}\")\n",
    "\n",
    "# 2. Logistic Regression (Grid Search)\n",
    "params_lr = {'C': [0.1, 1, 10, 100], 'solver': ['lbfgs', 'liblinear']}\n",
    "grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), params_lr, scoring='neg_log_loss', cv=5)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "y_pred_lr = grid_lr.predict_proba(X_test)\n",
    "loss_lr = log_loss(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Log Loss: {loss_lr:.4f} (Best Params: {grid_lr.best_params_})\")\n",
    "\n",
    "# 3. Gradient Boosting (Grid Search)\n",
    "params_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [20, 50],\n",
    "    'max_depth': [2, 3, None]\n",
    "}\n",
    "grid_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), params_gb, scoring='neg_log_loss', cv=5)\n",
    "grid_gb.fit(X_train, y_train)\n",
    "y_pred_gb = grid_gb.predict_proba(X_test)\n",
    "loss_gb = log_loss(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Log Loss: {loss_gb:.4f} (Best Params: {grid_gb.best_params_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d346f",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load Data\n",
    "df_sac = pd.read_csv(\"Sacremento.csv\")\n",
    "df_sac = df_sac.drop(['city', 'zip'], axis=1) # Ignore zip and city\n",
    "\n",
    "# Preprocessing\n",
    "X = df_sac.drop('price', axis=1)\n",
    "y = df_sac['price']\n",
    "\n",
    "categorical_cols = ['type']\n",
    "numerical_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Ridge Regression\n",
    "pipeline_ridge = Pipeline([('pre', preprocessor), ('model', Ridge())])\n",
    "params_ridge = {'model__alpha': [0, 0.1, 1, 1.5, 2]}\n",
    "grid_ridge = GridSearchCV(pipeline_ridge, params_ridge, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "mse_ridge = mean_squared_error(y_test, grid_ridge.predict(X_test))\n",
    "print(f\"Ridge MSE: {mse_ridge:.2f}\")\n",
    "\n",
    "# 2. Decision Tree\n",
    "pipeline_dt = Pipeline([('pre', preprocessor), ('model', DecisionTreeRegressor(random_state=42))])\n",
    "params_dt = {'model__max_depth': [3, 5], 'model__min_samples_split': [4, 10], 'model__min_samples_leaf': [2, 5]}\n",
    "grid_dt = GridSearchCV(pipeline_dt, params_dt, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "mse_dt = mean_squared_error(y_test, grid_dt.predict(X_test))\n",
    "print(f\"Decision Tree MSE: {mse_dt:.2f}\")\n",
    "\n",
    "# 3. Random Forest\n",
    "pipeline_rf = Pipeline([('pre', preprocessor), ('model', RandomForestRegressor(random_state=42))])\n",
    "params_rf = {'model__max_features': [4, 6, 9]}\n",
    "grid_rf = GridSearchCV(pipeline_rf, params_rf, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "mse_rf = mean_squared_error(y_test, grid_rf.predict(X_test))\n",
    "print(f\"Random Forest MSE: {mse_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b99b37",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Data\n",
    "df_arrests = pd.read_csv(\"USArrests.csv\")\n",
    "df_arrests.rename(columns={df_arrests.columns[0]: 'State'}, inplace=True)\n",
    "df_arrests.set_index('State', inplace=True)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_arrests)\n",
    "\n",
    "# 1. K-Means Clustering & Elbow Plot\n",
    "wss = []\n",
    "K_range = range(3, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, wss, marker='o')\n",
    "plt.title('Elbow Method for USArrests')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WSS')\n",
    "plt.show()\n",
    "\n",
    "# 2. PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(\"Cumulative Variance Ratio:\", cum_var)\n",
    "n_components = np.argmax(cum_var >= 0.70) + 1\n",
    "print(f\"Number of components capturing >= 70% variation: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db66afd",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load Data\n",
    "df_bb = pd.read_csv(\"BUNDESBANK-BBK01_WT5511.csv\")\n",
    "df_bb['Date'] = pd.to_datetime(df_bb['Date'], format='%d-%m-%Y')\n",
    "df_bb.set_index('Date', inplace=True)\n",
    "df_bb.sort_index(inplace=True)\n",
    "\n",
    "# Split Train/Test (Last 10 values for test)\n",
    "train = df_bb.iloc[:-10]\n",
    "test = df_bb.iloc[-10:]\n",
    "\n",
    "# 1. Holt’s Linear Trend\n",
    "model_holt = Holt(train['Value']).fit()\n",
    "pred_holt = model_holt.forecast(10)\n",
    "mse_holt = mean_squared_error(test['Value'], pred_holt)\n",
    "print(f\"Holt's Linear Trend MSE: {mse_holt:.2f}\")\n",
    "\n",
    "# 2. Holt-Winter’s Method (Additive Seasonality)\n",
    "# Assuming monthly data, seasonal_periods=12\n",
    "model_hw = ExponentialSmoothing(train['Value'], trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "pred_hw = model_hw.forecast(10)\n",
    "mse_hw = mean_squared_error(test['Value'], pred_hw)\n",
    "print(f\"Holt-Winter's Additive MSE: {mse_hw:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train.index[-50:], train['Value'].iloc[-50:], label='Train')\n",
    "plt.plot(test.index, test['Value'], label='Test')\n",
    "plt.plot(test.index, pred_holt, label='Holt Forecast')\n",
    "plt.plot(test.index, pred_hw, label='HW Additive Forecast')\n",
    "plt.legend()\n",
    "plt.title('Bundesbank Forecast Comparison')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
